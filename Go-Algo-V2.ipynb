{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38022a6",
   "metadata": {},
   "source": [
    "# Alpha Go Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7c3b8",
   "metadata": {},
   "source": [
    "In beating 18x Go World Champion, Lee Sedol, Alpha Go became the first computer player to attain the highest possible certification of 9 dan. More than that, it inspired a new wave of possibilities for the application of Machine Learning/Artifical Intelligence models, shattering the glass ceiling that was the previously thought limit to a computer's creativity. Here I implement a basic 9x9 version of Alpha Go's model using a similar combination of two neural networks (policy and value), a residual network model structure, and supervised training on labeled 9x9 go games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c60700",
   "metadata": {},
   "source": [
    "Author: Jason Katz (June 2025 - August 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74761f8",
   "metadata": {},
   "source": [
    "### Step 1: Go Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2396a8",
   "metadata": {},
   "source": [
    "Playing Go requires an engine for representing the game's state and performing actions like making moves or calculating score. Below is an implementation of several critical methods such as \"move\", \"_calculate_liberties\" and \"get_legal_actions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, rows, cols):\n",
    "        self._last_black_move = None\n",
    "        self._last_white_move = None\n",
    "        self._blacks_prisoners = 0\n",
    "        self._whites_prisoners = 0\n",
    "        self._rows = rows\n",
    "        self._cols = cols\n",
    "        self.player = 1\n",
    "        self._ko = None\n",
    "        self._ko_player = None\n",
    "        self._board = np.zeros((rows, cols))\n",
    "        self._turn_board = np.zeros((rows, cols))\n",
    "        self.turn = 1\n",
    "        \n",
    "\n",
    "    def get_board(self):\n",
    "        return self._board\n",
    "\n",
    "\n",
    "    def get_rows(self):\n",
    "        return self._rows\n",
    "\n",
    "\n",
    "    def get_cols(self):\n",
    "        return self._cols\n",
    "\n",
    "\n",
    "    def percent_filled(self):\n",
    "        print(np.count_nonzero(self._board))\n",
    "        return np.count_nonzero(self._board) / (self._rows * self._cols)\n",
    "    \n",
    "    \n",
    "    def _coord_outside_board(self, row, col):\n",
    "        if row < 0 or row >= self._rows or col < 0 or col >= self._cols:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def _get_connected(self, row, col, board=None):\n",
    "        board = board if board is not None else self._board\n",
    "        if self._coord_outside_board(row, col):\n",
    "            raise ValueError(\"(\", row, \",\", col, \") is outside the bounds of the board; Cannot calculate liberties\")\n",
    "            \n",
    "        chain = {(row, col)}\n",
    "        neighbors = deque([(row+1, col), (row-1, col), (row, col+1), (row, col-1)])\n",
    "        visited = {(row, col)}\n",
    "        player = board[row][col]\n",
    "        while len(neighbors) > 0:\n",
    "            n_row, n_col = neighbors.popleft() # queue\n",
    "            if self._coord_outside_board(n_row, n_col) or (n_row, n_col) in visited:\n",
    "                continue\n",
    "            visited.add((n_row, n_col))\n",
    "            if board[n_row][n_col] == player:\n",
    "                chain.add((n_row, n_col))\n",
    "                new_neighbors = [(n_row+1, n_col), (n_row-1, n_col), (n_row, n_col+1), (n_row, n_col-1)]\n",
    "                neighbors.extend(new_neighbors)\n",
    "        return chain\n",
    "    \n",
    "    \n",
    "    def _calculate_liberties(self, chain, board=None):\n",
    "        board = board if board is not None else self._board\n",
    "        visited, liberties = set(), set()\n",
    "        for (row, col) in chain:\n",
    "            visited.add((row, col))\n",
    "            neighbors = deque([(row+1, col), (row-1, col), (row, col+1), (row, col-1)])\n",
    "            for (n_row, n_col) in neighbors:\n",
    "                if (n_row, n_col) in visited:\n",
    "                    continue\n",
    "                visited.add((n_row, n_col))\n",
    "                if not self._coord_outside_board(n_row, n_col) and board[n_row][n_col] == 0:\n",
    "                    liberties.add((n_row, n_col))\n",
    "                \n",
    "        return len(liberties)\n",
    "           \n",
    "        \n",
    "    def get_legal_actions(self): #max 82 moves\n",
    "        legal_actions = [\"pass\"]\n",
    "        \n",
    "        for row in range(self._rows):\n",
    "            for col in range(self._cols):\n",
    "                if (row, col) == self._ko and self._ko_player == -self.player:\n",
    "                    continue\n",
    "                elif self._board[row][col] != 0:\n",
    "                    continue\n",
    "                    \n",
    "                board_copy = copy.deepcopy(self._board)\n",
    "                board_copy[row][col] = self.player\n",
    "                chain = self._get_connected(row, col, board=board_copy)\n",
    "                if self._calculate_liberties(chain, board=board_copy) != 0:\n",
    "                    legal_actions.append((row, col))\n",
    "                    continue\n",
    "                else:\n",
    "                    for dr, dc in [(1,0), (-1,0), (0,1), (0,-1)]:\n",
    "                        r, c = row + dr, col + dc\n",
    "                        if 0 <= r < self._rows and 0 <= c < self._cols:\n",
    "                            if board_copy[r][c] == -self.player:\n",
    "                                opp_chain = self._get_connected(r, c, board=board_copy)\n",
    "                                if self._calculate_liberties(opp_chain, board=board_copy) == 0:\n",
    "                                    legal_actions.append((row, col))\n",
    "                                    break\n",
    "        print(f\"{len(legal_actions)} found for player: {self.player}\")\n",
    "        return legal_actions\n",
    "\n",
    "    \n",
    "    def is_game_over(self, board=None):\n",
    "        board = board if board is not None else self._board\n",
    "        if self._last_black_move == \"pass\" and self._last_white_move == \"pass\":\n",
    "            return True\n",
    "        for row in range(self._rows):\n",
    "            for col in range(self._cols):\n",
    "                if board[row][col] == 0:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def _calculate_territories(self, area_scoring, board=None): \n",
    "        # Note: the concept of stones being \"dead\" is ignored in this implementation\n",
    "        board = board if board is not None else self._board\n",
    "        black_territories, white_territories = 0, 0\n",
    "        visited = set()\n",
    "        for row in range(self._rows):\n",
    "            for col in range(self._cols):\n",
    "                if (row, col) in visited:\n",
    "                    continue\n",
    "                if board[row][col] == 0:\n",
    "                    group = self._get_connected(row, col, board)\n",
    "                    neighbors, contested = set(), False\n",
    "                    for (r, c) in group:\n",
    "                        for (dr, dc) in [(1,0), (-1,0), (0,1), (0,-1)]:\n",
    "                            if 0 <= r + dr < self._rows and 0 <= c + dc < self._cols:\n",
    "                                n = board[r+dr][c+dc]\n",
    "                                if n == 0:\n",
    "                                    continue\n",
    "                                neighbors.add(n)\n",
    "                                if len(neighbors) > 1:\n",
    "                                    contested = True\n",
    "                                    break\n",
    "                        if contested:\n",
    "                            break\n",
    "                    if len(neighbors) == 1:\n",
    "                        player = next(iter(neighbors))\n",
    "                        if player == 1:\n",
    "                            black_territories += len(group)\n",
    "                        else:\n",
    "                            white_territories += len(group)\n",
    "                    visited.update(group)\n",
    "                    \n",
    "                elif area_scoring and (row, col) not in visited:\n",
    "                    group = self._get_connected(row, col, board)\n",
    "                    if board[row][col] == 1:\n",
    "                        black_territories += len(group)\n",
    "                    else:\n",
    "                        white_territories += len(group)\n",
    "                    visited.update(group)\n",
    "        return {\"Black\": black_territories, \"White\": white_territories}\n",
    "    \n",
    "    \n",
    "    def _get_newest_turn(self, chain, board=None, turn_board=None):\n",
    "        board = board if board is not None else self._board\n",
    "        turn_board = turn_board if turn_board is not None else self._turn_board\n",
    "        newest = 1\n",
    "        for (r, c) in chain:\n",
    "            if self._turn_board[r][c] > newest:\n",
    "                newest = self._turn_board[r][c]\n",
    "        return newest\n",
    "            \n",
    "    \n",
    "    def _capture_prisoners(self, board=None, turn_board=None):\n",
    "        whites_prisoners, blacks_prisoners = 0,0\n",
    "        board = board if board is not None else self._board\n",
    "        turn_board = turn_board if turn_board is not None else self._turn_board\n",
    "        visited, to_remove = set(), set()\n",
    "        \n",
    "        for row in range(self._rows):\n",
    "            for col in range(self._cols):\n",
    "                if (row, col) not in visited and board[row][col] != 0:\n",
    "                    candidate_group = self._get_connected(row, col, board)\n",
    "                    visited.update(candidate_group)\n",
    "\n",
    "                    if self._calculate_liberties(candidate_group, board) == 0:\n",
    "                        group_timestamp = self._get_newest_turn(candidate_group, board, turn_board)\n",
    "                        this_player = board[row][col]\n",
    "                        remove_this_group = False\n",
    "                        visited_neighbors = set()\n",
    "                        \n",
    "                        for (r, c) in candidate_group:\n",
    "                            for (dr, dc) in [(1,0), (-1,0), (0,1), (0,-1)]:\n",
    "                                nr, nc = r + dr, c + dc\n",
    "                                if 0 <= nr < self._rows and 0 <= nc < self._cols:\n",
    "                                    if (nr, nc) not in candidate_group and (nr, nc) not in visited_neighbors and board[nr][nc] == -this_player:\n",
    "                                        opponent_group = self._get_connected(nr, nc, board)\n",
    "                                        visited_neighbors.update(opponent_group)\n",
    "                                        visited.update(opponent_group)\n",
    "                                        \n",
    "                                        if self._calculate_liberties(opponent_group, board) == 0: # ONE NEEDS TO GO\n",
    "                                            neighbor_timestamp = self._get_newest_turn(opponent_group, board, turn_board)\n",
    "                                            if neighbor_timestamp < group_timestamp:  # remove opponent_group\n",
    "                                                to_remove.update(opponent_group)\n",
    "                                            else:  # remove candidate_group and stop visiting neighbors\n",
    "                                                to_remove.update(candidate_group)\n",
    "                                                remove_this_group = True\n",
    "                                            \n",
    "                                            if remove_this_group:\n",
    "                                                break   \n",
    "                                        \n",
    "                            if remove_this_group:\n",
    "                                break\n",
    "                        if len(to_remove) == 0:\n",
    "                            to_remove.update(candidate_group)\n",
    "                        \n",
    "        # remove stones\n",
    "        for (r, c) in to_remove:\n",
    "            if board[r][c] == 1:\n",
    "                whites_prisoners += 1\n",
    "            else:\n",
    "                blacks_prisoners += 1\n",
    "                \n",
    "            board[r][c] = 0\n",
    "            turn_board[r][c] = 0\n",
    "\n",
    "        return {\"Black\":blacks_prisoners, \"White\":whites_prisoners}\n",
    "        \n",
    "        \n",
    "    def pass_turn(self):\n",
    "        if self.player == 1:\n",
    "            self._last_black_move = \"pass\"\n",
    "        else:\n",
    "            self._last_white_move = \"pass\"\n",
    "        self.player *= -1\n",
    "                    \n",
    "                \n",
    "    def move(self, row, col): # no simulations with move\n",
    "        if row < 0 or row >= self._rows or col < 0 or col >= self._cols:\n",
    "            raise ValueError(\"Invalid move: coordinate outside board\")\n",
    "        elif self._board[row][col] != 0:\n",
    "            raise ValueError(\"Invalid move: coordinate not empty\")\n",
    "        elif (row, col) == self._ko and self._ko_player == -self.player:\n",
    "            raise ValueError(\"Invalid move: Ko\")\n",
    "        newKo_r, newKo_c = None, None\n",
    "        \n",
    "        can_add = False\n",
    "        \n",
    "        # not immediately captured\n",
    "        board_copy = copy.deepcopy(self._board)\n",
    "        board_copy[row][col] = self.player\n",
    "        chain = self._get_connected(row, col, board=board_copy)\n",
    "        if self._calculate_liberties(chain, board=board_copy) != 0:\n",
    "            can_add = True\n",
    "            \n",
    "        if not can_add: # captures an opponent piece\n",
    "            for dr, dc in [(1,0), (-1,0), (0,1), (0,-1)]:\n",
    "                r, c = row + dr, col + dc\n",
    "                if 0 <= r < self._rows and 0 <= c < self._cols:\n",
    "                    if board_copy[r][c] == -self.player:\n",
    "                        opp_chain = self._get_connected(r, c, board=board_copy)\n",
    "                        if self._calculate_liberties(opp_chain, board=board_copy) == 0:\n",
    "                            can_add = True\n",
    "                            if len(opp_chain) == 1:\n",
    "                                newKo_r = r\n",
    "                                newKo_c = c\n",
    "                            break\n",
    "\n",
    "        if not can_add:\n",
    "            raise ValueError(\"Invalid move: immediate capture\")\n",
    "                            \n",
    "        self._board[row][col] = self.player\n",
    "        self._turn_board[row][col] = self.turn\n",
    "        self.turn+=1\n",
    "        \n",
    "        if self.player == 1:\n",
    "            self._last_black_move = (row, col)\n",
    "        else:\n",
    "            self._last_white_move = (row, col)\n",
    "\n",
    "        new_prisoners = self._capture_prisoners()\n",
    "        blacks_prisoners, whites_prisoners = new_prisoners[\"Black\"], new_prisoners[\"White\"]\n",
    "        self._blacks_prisoners += blacks_prisoners\n",
    "        self._whites_prisoners += whites_prisoners\n",
    "        \n",
    "        \n",
    "        if blacks_prisoners + whites_prisoners == 1:\n",
    "            self._ko = (newKo_r, newKo_c)\n",
    "            self._ko_player = self.player\n",
    "        else:\n",
    "            self._ko = None\n",
    "            self._ko_player = None\n",
    "        self.player *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3039c4",
   "metadata": {},
   "source": [
    "### Step 2: Board to Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ef0e5",
   "metadata": {},
   "source": [
    "With the Go engine complete, we now need a way to convert our representation of the board's state to a tensor—a nested array that is *mostly* one-hot encoded. I've opted to use 10 channels, each of which have a uniform 9x9 shape. These channels are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb295ced",
   "metadata": {},
   "source": [
    "**Channel 0: Current Player's Stones** A 1 in (row, col) means a stone exists in that location, a 0 indicates otherwise.\n",
    "\n",
    "**Channel 1: Opponent Player's Stones** A 1 in (row, col) means that an opposing stone exists in that location, a 0 indicates otherwise.\n",
    "\n",
    "**Channel 2: Empty Positions** A 1 in (row, col) means that the space is empty, a 0 means it is filled.\n",
    "\n",
    "**Channel 3: Ko Position** A 1 in (row, col) means that is the active Ko position. There can be at most 1 Ko position.\n",
    "\n",
    "**Channel 4: Ko Player** The only relevant location in this 9x9 array is (0,0). A 1 in this location indicates that black was the player meaning to impose Ko (white cannot move here); alternatively, a 0 in this location indicates that white imposed the Ko.\n",
    "\n",
    "**Channel 5: Current Player Turn** All 1's means it is black's turn; All 0's mean it's white's.\n",
    "\n",
    "**Channel 6: Last Black Move** A 1 in (row, col) means this was black's last move. Black can have at most 1 last move.\n",
    "\n",
    "**Channel 7: Last White Move** A 1 in (row, col) means this was white's last move. White can have at most 1 last move.\n",
    "\n",
    "**Channel 8: Turn Board** This channel is *not* one-hot encoded. The number in (row, col) indicates which turn that stone was placed.\n",
    "\n",
    "**Channel 9: Turn Number** (0,0) is the only relevant board location. This channel is also not one-hot encoded, with the number at (0,0) representing the current turn number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8821cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_tensor(board_instance):\n",
    "    board = board_instance._board\n",
    "    ko = board_instance._ko # (r,c)\n",
    "    ko_player = board_instance._ko_player # 1 or -1\n",
    "    player = board_instance.player # 1 or -1\n",
    "    rows, cols = board.shape # 9,9\n",
    "    last_black_move = board_instance._last_black_move # (r,c)\n",
    "    last_white_move = board_instance._last_white_move # (r,c)\n",
    "    turn_board = board_instance._turn_board\n",
    "    turn = board_instance.turn # int\n",
    "    tensor = np.zeros((10, rows, cols), dtype = np.float32)\n",
    "    \n",
    "    # Channels 0-2 are for board pieces\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if board[r][c] == player:\n",
    "                tensor[0][r][c] = 1  # current player's stones\n",
    "            elif board[r][c] == -player:\n",
    "                tensor[1][r][c] = 1  # opponent stones\n",
    "            elif board[r][c] == 0:\n",
    "                tensor[2][r][c] = 1  # empty positions\n",
    "                \n",
    "    # Channels 3-4 are for Ko     \n",
    "    if ko:\n",
    "        ko_r, ko_c = ko\n",
    "        tensor[3][ko_r][ko_c] = 1  # ko position\n",
    "        if ko_player == 1:\n",
    "            tensor[4][:, :] = 1\n",
    "        else:\n",
    "            tensor[4][:, :] = -1\n",
    "    \n",
    "    # Channel 5 is for the player's turn it is next\n",
    "    if player == 1:\n",
    "        tensor[5][:, :] = 1  # 1 for black\n",
    "    else:\n",
    "        tensor[5][:, :] = 0  # 0 for white\n",
    "    \n",
    "    # Channel 6-7 is for last black/white moves\n",
    "    tensor[6][:, :] = 0\n",
    "    tensor[7][:, :] = 0\n",
    "    if last_black_move:\n",
    "        r, c = last_black_move\n",
    "        tensor[6][r][c] = 1\n",
    "    if last_white_move:\n",
    "        r, c = last_white_move\n",
    "        tensor[7][r][c] = 1\n",
    "    \n",
    "    # Channel 8 is for tracking when pieces were placed\n",
    "    tensor[8] = turn_board\n",
    "    \n",
    "    # Channel 9 is for the current turn #\n",
    "    tensor[9][0][0] = turn\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ac850",
   "metadata": {},
   "source": [
    "### ResNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7e889",
   "metadata": {},
   "source": [
    "Below is a simple implementation of a Residual Network (ResNet) architecture used to create the policy and value networks. The policy network combines PyTorch's CNNs with ReLu activation to create an output array of size 81 (representing all possible moves) and their resulting win probabilities. The value network takes a similar approach and outputs a scalar value representing the expected win probability from the current board state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1122f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SimpleResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = out + residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(in_channels=10, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.res_block1 = SimpleResidualBlock(32)\n",
    "        self.res_block2 = SimpleResidualBlock(32)\n",
    "        \n",
    "        self.policy_conv = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=1)\n",
    "        self.policy_fc1 = nn.Linear(in_features=(2 * 9 * 9), out_features=128)\n",
    "        self.policy_fc2 = nn.Linear(in_features=128, out_features=(9 * 9 * 1))\n",
    "        \n",
    "        self.value_conv = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)\n",
    "        self.value_fc1 = nn.Linear(in_features=(1 * 9 * 9), out_features=64)\n",
    "        self.value_fc2 = nn.Linear(in_features=64, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = self.initial_conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        \n",
    "        policy = self.policy_conv(x)\n",
    "        policy = F.relu(policy)\n",
    "        policy = policy.view(batch_size, -1) # reshapes to (batch_size, 162)\n",
    "        policy = self.policy_fc1(policy)\n",
    "        policy = F.relu(policy)\n",
    "        policy_logits = self.policy_fc2(policy)\n",
    "                                    \n",
    "        \n",
    "        print(f\"Policy logits shape: {policy_logits.shape}\")\n",
    "        \n",
    "        value = self.value_conv(x)\n",
    "        value = F.relu(value)\n",
    "        value = value.view(batch_size, -1) # reshapes to (batch_size, 81)\n",
    "        value = self.value_fc1(value)\n",
    "        value = F.relu(value)\n",
    "        value = self.value_fc2(value)\n",
    "        value = torch.tanh(value) # ensures values are in range [-1, 1] (W/L)\n",
    "        print(f\"Value shape: {value.shape}\")\n",
    "        \n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfbe2f",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3ffbf",
   "metadata": {},
   "source": [
    "I've downloaded many games of Go in SGF format from https://github.com/gto76/online-go-games/blob/master/games.zip.005. To train our model, we first need to parse these SGF files into an accepted form of training data (tensors). Part of the data cleaning process including filtering out games that were not played on a 9x9 board as well as those where the player's rank was below 9k. This parsing and chunking of the resulting dataset was computed on my local CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9af51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sgfmill import sgf\n",
    "\n",
    "\n",
    "# Parse a single SGF file and generate training samples\n",
    "def extract_data_from_sgf_custom(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            game = sgf.Sgf_game.from_bytes(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file_path}: {e}\")\n",
    "        return [], None\n",
    "\n",
    "    # Skip non-9x9 boards\n",
    "    if game.get_size() != 9:\n",
    "        return [], None\n",
    "    \n",
    "    try:  \n",
    "        black_rank = game.get_root().get('BR')\n",
    "        white_rank = game.get_root().get('WR')\n",
    "        if len(black_rank) > 2 or len(white_rank) > 2:\n",
    "            return [], None\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file_path}: {e}\")\n",
    "        return [], None\n",
    "            \n",
    "    print(f\"Black Rank: {black_rank}\")\n",
    "\n",
    "    board = Board(9, 9)\n",
    "    samples = []\n",
    "\n",
    "    for node in game.get_main_sequence():\n",
    "        move = node.get_move()\n",
    "        color, coords = move\n",
    "\n",
    "        if coords is None:\n",
    "            continue  # Skip pass moves\n",
    "\n",
    "        row, col = coords\n",
    "        player = 1 if color == 'b' else -1\n",
    "\n",
    "        board_tensor = board_to_tensor(board)\n",
    "        move_index = row * 9 + col\n",
    "        samples.append((board_tensor, move_index))\n",
    "        try:\n",
    "            board.move(row, col)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "    winner = game.get_winner()\n",
    "    result = 1 if winner == 'b' else -1 if winner == 'w' else 0\n",
    "    return samples, result\n",
    "\n",
    "\n",
    "# Process all SGF files in a directory\n",
    "def process_all_sgfs_custom(sgf_dir):\n",
    "    X, y_policy, y_value = [], [], []\n",
    "\n",
    "    buffer_X, buffer_policy, buffer_value = [], [], []\n",
    "    counter = 0\n",
    "    chunk_size = 100\n",
    "    chunk_index = 0\n",
    "\n",
    "    for filename in os.listdir(sgf_dir):\n",
    "        \n",
    "        if not filename.endswith(\".sgf\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(sgf_dir, filename)\n",
    "        game_data, result = extract_data_from_sgf_custom(path)\n",
    "\n",
    "        if game_data and result is not None:\n",
    "            print(counter)\n",
    "            for board_tensor, move_idx in game_data:\n",
    "                buffer_X.append(torch.tensor(board_tensor))\n",
    "                buffer_policy.append(move_idx)\n",
    "                buffer_value.append(result)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        # Save chunk if ready\n",
    "        if counter % chunk_size == 0 and len(buffer_X) > 0:\n",
    "            print(f\"Saving chunk {chunk_index} with {len(buffer_X)} samples\")\n",
    "            torch.save({\n",
    "                'X': torch.stack(buffer_X),\n",
    "                'y_policy': torch.tensor(buffer_policy),\n",
    "                'y_value': torch.tensor(buffer_value, dtype=torch.float32)\n",
    "            }, f\"dataset_chunk_{chunk_index}.pt\")\n",
    "\n",
    "            chunk_index += 1\n",
    "            buffer_X, buffer_policy, buffer_value = [], [], []\n",
    "            \n",
    "        if counter > 200000:\n",
    "            break\n",
    "\n",
    "    # Final save if anything is left over\n",
    "    if len(buffer_X) > 0:\n",
    "        print(f\"Saving final chunk {chunk_index} with {len(buffer_X)} samples\")\n",
    "        torch.save({\n",
    "            'X': torch.stack(buffer_X),\n",
    "            'y_policy': torch.tensor(buffer_policy),\n",
    "            'y_value': torch.tensor(buffer_value, dtype=torch.float32)\n",
    "        }, f\"dataset_chunk_{chunk_index}.pt\")\n",
    "\n",
    "    return None, None, None  # if saving in chunks, we don't return a full in-memory dataset\n",
    "\n",
    "# Save to .pt\n",
    "def save_dataset(path, X, y_policy, y_value):\n",
    "    dataset = {\"X\": X, \"y_policy\": y_policy, \"y_value\": y_value}\n",
    "    torch.save(dataset, path)\n",
    "\n",
    "sgf_dir = \"[FILL ME IN]\"\n",
    "# sgf_dir = \"/Users/jasonkatz/Desktop/pro-go-games\"\n",
    "output_path = \"[FILL ME IN]/pro_go_dataset.pt\"\n",
    "# output_path = \"/Users/jasonkatz/Desktop/test-games/go_dataset_custom.pt\"\n",
    "\n",
    "X, y_policy, y_value = process_all_sgfs_custom(sgf_dir)\n",
    "save_dataset(output_path, X, y_policy, y_value)\n",
    "print(f\"Saved {len(X)} training samples to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efda82",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627533f",
   "metadata": {},
   "source": [
    "To train the model, we load each chunked dataset, instantiate our model, and iteratively update a policy loss and value loss after each pass through the set of labeled data. The resulting weights are ultimately saved to a \".pt\" file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GoChunkedDataset(Dataset):\n",
    "    def __init__(self, chunk_folder):\n",
    "        self.chunk_paths = [\n",
    "            os.path.join(chunk_folder, f)\n",
    "            for f in os.listdir(chunk_folder)\n",
    "            if f.endswith(\".pt\")\n",
    "        ]\n",
    "        \n",
    "        self.samples = []  # (file_idx, local_idx) mapping\n",
    "\n",
    "        # Load file sizes\n",
    "        self.chunk_metadata = []\n",
    "        for i, path in enumerate(self.chunk_paths):\n",
    "            data = torch.load(path)\n",
    "            size = len(data['X'])\n",
    "            self.chunk_metadata.append((i, size))\n",
    "            for j in range(size):\n",
    "                self.samples.append((i, j))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, local_idx = self.samples[idx]\n",
    "        data = torch.load(self.chunk_paths[file_idx])\n",
    "        return (\n",
    "            data['X'][local_idx],          # [10, 9, 9]\n",
    "            data['y_policy'][local_idx],   # int (0–80)\n",
    "            data['y_value'][local_idx]     # float (-1 to 1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = GoChunkedDataset(\"[FILL ME IN]\")\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "policy_loss_fn = nn.CrossEntropyLoss()\n",
    "value_loss_fn = nn.MSELoss()\n",
    "model = ResNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for epoch in range(40):\n",
    "    total_loss = 0\n",
    "    for board_tensor, move_idx, game_result in dataloader:\n",
    "        board_tensor = board_tensor.to(device)\n",
    "        move_idx = move_idx.to(device)\n",
    "        game_result = game_result.to(device)\n",
    "\n",
    "        policy_logits, value_output = model(board_tensor)\n",
    "\n",
    "        loss_policy = policy_loss_fn(policy_logits, move_idx)\n",
    "        loss_value = value_loss_fn(value_output.squeeze(), game_result)\n",
    "\n",
    "        loss = loss_policy + loss_value\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ef55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet_go_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(\"[FILL ME IN]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load your combined dataset (or use a chunk)\n",
    "data = torch.load(\"dataset_chunk_0.pt\")  # or \"dataset_chunk_0.pt\"\n",
    "\n",
    "X = data['X']\n",
    "y_policy = data['y_policy']\n",
    "y_value = data['y_value']\n",
    "\n",
    "model = ResNet()\n",
    "model.load_state_dict(torch.load(\"resnet_go_pretrained.pt\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "correct_policy = 0\n",
    "total = 0\n",
    "mse_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X)):\n",
    "        board_tensor = X[i].unsqueeze(0)  # Add batch dimension [1, 10, 9, 9]\n",
    "        policy_target = y_policy[i]\n",
    "        value_target = y_value[i]\n",
    "\n",
    "        policy_logits, value_output = model(board_tensor)\n",
    "\n",
    "        # POLICY ACCURACY\n",
    "        predicted_move = policy_logits.argmax(dim=1).item()\n",
    "        if predicted_move == policy_target.item():\n",
    "            correct_policy += 1\n",
    "\n",
    "        # VALUE MSE\n",
    "        predicted_value = value_output.squeeze().item()\n",
    "        mse_loss += (predicted_value - value_target.item()) ** 2\n",
    "\n",
    "        total += 1\n",
    "\n",
    "# Final metrics\n",
    "accuracy = correct_policy / total\n",
    "mse = mse_loss / total\n",
    "\n",
    "print(f\"Evaluation Complete\")\n",
    "print(f\"Policy Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Value Head MSE:  {mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "go-algo",
   "language": "python",
   "name": "go-algo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
